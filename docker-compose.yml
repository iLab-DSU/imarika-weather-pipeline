version: '3.8'
services:
  spark-submit:
    build:
      context: .
      dockerfile: Dockerfile.spark
    container_name: imarika-spark-submit-${UNIQUE_ID}
    depends_on:
      kafka-broker-1:
        condition: service_healthy
      spark-master:
        condition: service_started
      postgres:
        condition: service_healthy
    volumes:
    - ${PWD}:/opt/spark-apps
    command:
      - /opt/bitnami/spark/bin/spark-submit
      - --master
      - spark://imarika-spark-master:7077
      - --packages
      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.postgresql:postgresql:42.5.1
      - --conf
      - spark.jars.ivy=/tmp/.ivy
      - /opt/spark-apps/weather_spark_processor.py
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-broker-1:19092
      KAFKA_TOPIC_RAW: weather-readings-raw
      POSTGRES_URL: jdbc:postgresql://imarika-postgres:5432/imarika
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      CHECKPOINT_LOCATION: /tmp/imarika/checkpoints
      MAX_OFFSETS_PER_TRIGGER: 1000
    networks:
    - confluent
  producer:
    build:
      context: .
      dockerfile: Dockerfile.producer
    container_name: api_data_producer
    depends_on:
    - kafka-broker-1
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-broker-1:19092
      EMAIL: derrick.lubanga@strathmore.edu
      PASSWORD: Expendables
    volumes:
    - ./api_data_fetcher_kafka.py:/app/api_data_fetcher_kafka.py
    - ./requirements.txt:/app/requirements.txt
    networks:
    - confluent
  consumer:
    build:
      context: .
      dockerfile: Dockerfile.consumer
    container_name: kafka_consumer
    depends_on:
    - kafka-broker-1
    environment:
      KAFKA_BOOTSTRAP_SERVERS: kafka-broker-1:19092
    volumes:
    - ./kafka_consumer.py:/app/kafka_consumer.py
    - ./requirements.txt:/app/requirements.txt
    networks:
    - confluent
